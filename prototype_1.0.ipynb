{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cbb55dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mediapipe as mp\n",
    "import cv2\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from Fonction import *\n",
    "import threading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b371d7e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_drawing_styles = mp.solutions.drawing_styles\n",
    "\n",
    "mp_hands = mp.solutions.hands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c1ffe237",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.models.load_model('test.h5')\n",
    "# model.load_weights('train_results/lastweights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7c35ece1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.output_shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "2e12faf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pycaw.pycaw import AudioUtilities, IAudioEndpointVolume\n",
    "\n",
    "\n",
    "def process():\n",
    "    global data, ancien_lm, derive, proba_liss, flag, time_cmd\n",
    "    # extraction des lms ########################################################\n",
    "    hand_lm = lm_extraction(hand_resultats)\n",
    "    lm_flat = hand_lm.copy()\n",
    "    lm_flat = lm_flat.flatten()\n",
    "\n",
    "    if hand_resultats.multi_hand_landmarks:\n",
    "        # Dessin des landmarks ########################################################\n",
    "        lm_drawing(image, hand_resultats)\n",
    "\n",
    "        # calcule d'angles ########################################################\n",
    "        angle = angle_process(hand_lm)\n",
    "        cv2.putText(pop, 'angle :'+str(round(angle[3]*180/np.pi)), org=(0, 600), fontFace=cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                    fontScale=0.8, color=(150, 255, 0), thickness=1)\n",
    "\n",
    "        # calcule d'angles ########################################################\n",
    "        dist = distance_process(hand_lm)\n",
    "        cv2.putText(pop, 'distance pouce/index :'+str(round(dist[6], 3)), org=(0, 650), fontFace=cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                    fontScale=0.8, color=(150, 255, 0), thickness=1)\n",
    "    else:\n",
    "        angle = np.zeros(15)\n",
    "        dist = np.zeros(21)\n",
    "    # mise en forme de la donnée (1,99) ########################################################\n",
    "    data_frame = np.concatenate((lm_flat, angle.flatten(), dist.flatten()))\n",
    "    # Sav des lm sur un vecteur temps de 20 echantion glissant\n",
    "    data = np.concatenate(\n",
    "        [data[1:], np.expand_dims(data_frame, axis=0)], axis=0)\n",
    "\n",
    "    derive_actu = hand_lm-ancien_lm\n",
    "    ancien_lm = hand_lm\n",
    "    derive = np.concatenate(\n",
    "        [derive[1:], np.expand_dims(derive_actu, axis=0)], axis=0)\n",
    "\n",
    "    if hand_resultats.multi_hand_landmarks:\n",
    "        id_hand = hand_resultats.multi_handedness[0].classification[0].label\n",
    "        # mise en forme pour prédiction (1,99) ########################################################\n",
    "        lmr = np.reshape(data, (1, 20, 99))\n",
    "\n",
    "        # classification et calcul de temps ########################################################\n",
    "        spred = time.time()\n",
    "        vector_pred = model.predict_on_batch(lmr)[0]\n",
    "        timepred = round((time.time()-spred)*1000, 3)\n",
    "\n",
    "        # enregistrement de l'historique de prediction\n",
    "        proba_liss = np.concatenate(\n",
    "            [proba_liss[1:], np.expand_dims(vector_pred, axis=0)], axis=0)\n",
    "        prob_filtre = np.mean(proba_liss, axis=0)\n",
    "\n",
    "        # affichage du temps de prediction ########################################################\n",
    "        cv2.putText(pop, 'temps classification :' + str(timepred)+\" ms\", org=(201, 120), fontFace=cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                    fontScale=0.5, color=(0, 255, 0), thickness=1)\n",
    "\n",
    "        # AFFICHAGE ET TRAITEMENT DE LA PREDICTION  ########################################################\n",
    "        # pred=np.argmax(vector_pred)\n",
    "        # print(vector_pred)\n",
    "        # print(pred,\" --> confidence ---> \",vector_pred[pred])\n",
    "        proba_viz(prob_filtre, pop)\n",
    "\n",
    "        # CALCUL DE VITESSE DE L'Index\n",
    "        index_y_derive = np.mean(derive[50:-1, 8, 1])\n",
    "        index_y_derive = index_y_derive*-1\n",
    "        if vector_pred[10] > 0.85:\n",
    "            volume = get_master_volume()\n",
    "            mouv_v = round(index_y_derive*50, 2)\n",
    "            if (time.time()-time_cmd) > 0.6:\n",
    "                # Seuillage de l'activation\n",
    "                if mouv_v > -0.5 and mouv_v < 0.5:\n",
    "                    mouv_v = 0\n",
    "\n",
    "                # Limitation de la modif de volume\n",
    "                if mouv_v > 70:\n",
    "                    mouv_v = 70\n",
    "                if mouv_v < -50:\n",
    "                    mouv_v = -50\n",
    "                mouv_v = mouv_v/10\n",
    "                new_vol = mouv_v+volume\n",
    "\n",
    "                # limiation pour eviter les erreur systeme\n",
    "                if new_vol > 1:\n",
    "                    new_vol = 1\n",
    "                elif new_vol < 0:\n",
    "                    new_vol = 0\n",
    "\n",
    "                set_master_volume(new_vol)\n",
    "                print('volume : ', new_vol)\n",
    "                time_cmd = time.time()\n",
    "            cv2.putText(pop, 'vitesse du doigt :' + str(mouv_v), org=(201, 150), fontFace=cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                        fontScale=0.5, color=(0, 255, 0), thickness=1)\n",
    "    else:\n",
    "        id_hand = 'No hand'\n",
    "\n",
    "    cv2.putText(pop, id_hand, org=(201, 160), fontFace=cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                fontScale=0.5, color=(0, 255, 0), thickness=1)\n",
    "    cv2.imshow('Resultat de détection', pop)\n",
    "    cv2.imshow('hand Model', image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "1bf0766b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\manon\\miniconda3\\envs\\env_ziane\\Lib\\site-packages\\google\\protobuf\\symbol_database.py:55: UserWarning: SymbolDatabase.GetPrototype() is deprecated. Please use message_factory.GetMessageClass() instead. SymbolDatabase.GetPrototype() will be removed soon.\n",
      "  warnings.warn('SymbolDatabase.GetPrototype() is deprecated. Please '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "volume :  0.7210000157356262\n",
      "volume :  0.7210000157356262\n",
      "volume :  0.7210000157356262\n"
     ]
    }
   ],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "cap.set(cv2.CAP_PROP_FRAME_WIDTH, 1920/3)\n",
    "cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 1080/3)\n",
    "cap.set(cv2.CAP_PROP_FPS, 25)\n",
    "\n",
    "# Parametre holitic\n",
    "time_act = time.time()\n",
    "hands = mp_hands.Hands(static_image_mode=False, max_num_hands=1)\n",
    "\n",
    "# Préparation des variable utils  ########################################################\n",
    "fps = 0\n",
    "fps_results = 0\n",
    "cTime = time.time()\n",
    "count = 0\n",
    "data = np.zeros((20, 99))\n",
    "derive = np.zeros((60, 21, 3))\n",
    "ancien_lm = np.zeros((21, 3))\n",
    "proba_liss = np.zeros((15, model.output_shape[1]))\n",
    "time_cmd = time.time()\n",
    "flag = 0\n",
    "\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    pop = np.zeros((800, 450, 3), dtype=np.uint8)\n",
    "\n",
    "    # switch de canal couleur\n",
    "    image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    # flip pour un effect mirroir\n",
    "    image = cv2.flip(image, 1)\n",
    "    # detection\n",
    "    image.flags.writeable = False\n",
    "    hand_resultats = hands.process(image)\n",
    "    image.flags.writeable = True\n",
    "    # Retour en format BGR\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "    fps += 1\n",
    "    if time.time()-cTime >= 1:\n",
    "        fps_results = fps\n",
    "        fps = 0\n",
    "        cTime = time.time()\n",
    "    # pTime=cTime\n",
    "    cv2.putText(image, 'fps :'+str(fps_results), org=(50, 200), fontFace=cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                fontScale=1, color=(0, 255, 0), thickness=2)\n",
    "\n",
    "    th1 = threading.Thread(target=process())\n",
    "    th1.start()\n",
    "    if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c2942f27",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e9e56b8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Volume actuel: 0.1599999964237213\n"
     ]
    }
   ],
   "source": [
    "from pycaw.pycaw import AudioUtilities, IAudioEndpointVolume\n",
    "from comtypes import CLSCTX_ALL\n",
    "def get_master_volume():\n",
    "    devices = AudioUtilities.GetSpeakers()\n",
    "    interface = devices.Activate(IAudioEndpointVolume._iid_, CLSCTX_ALL, None)\n",
    "    volume_object = interface.QueryInterface(IAudioEndpointVolume)\n",
    "    return volume_object.GetMasterVolumeLevelScalar()\n",
    "\n",
    "# Exemple d'utilisation\n",
    "current_volume = get_master_volume()\n",
    "print(\"Volume actuel:\", current_volume)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b322a45a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'classe_list' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#si la valeur de ça est superieur a un seuil\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[43mclasse_list\u001b[49m[np\u001b[38;5;241m.\u001b[39margmax(vector_pred)]\n",
      "\u001b[1;31mNameError\u001b[0m: name 'classe_list' is not defined"
     ]
    }
   ],
   "source": [
    "#si la valeur de ça est superieur a un seuil\n",
    "classe_list[np.argmax(vector_pred)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "accbd0bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "with open('./classe.csv') as mon_fichier:\n",
    "    mon_fichier_reader = csv.reader(mon_fichier, delimiter=',', quotechar='\"')\n",
    "    classe_list = [x for x in mon_fichier_reader]\n",
    "classe_list=classe_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d970087d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'hand_resultats' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mhand_resultats\u001b[49m\u001b[38;5;241m.\u001b[39mmulti_handedness[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mclassification[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mlabel\n",
      "\u001b[1;31mNameError\u001b[0m: name 'hand_resultats' is not defined"
     ]
    }
   ],
   "source": [
    "hand_resultats.multi_handedness[0].classification[0].label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b97286af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.        , 2.        , 2.66666667])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(np.array([[1,1,1],[2,2,2],[3,3,5]]),axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a27d469",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
